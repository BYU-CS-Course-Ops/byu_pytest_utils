versions pytest-7.4.3, python-3.11.5.final.0
cwd=/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests
args=('--debug',)

  pytest_cmdline_main [hook]
      config: <_pytest.config.Config object at 0x10c9d9790>
    pytest_plugin_registered [hook]
        plugin: <Session tests exitstatus='<UNSET>' testsfailed=0 testscollected=0>
        manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
    finish pytest_plugin_registered --> [] [hook]
    pytest_configure [hook]
        config: <_pytest.config.Config object at 0x10c9d9790>
    early skip of rewriting module: plistlib [assertion]
    early skip of rewriting module: xml.parsers [assertion]
    early skip of rewriting module: xml.parsers.expat [assertion]
    early skip of rewriting module: email.parser [assertion]
    early skip of rewriting module: email.feedparser [assertion]
      pytest_metadata [hook]
          metadata: {'Python': '3.11.5', 'Platform': 'macOS-10.16-x86_64-i386-64bit', 'Packages': {'pytest': '7.4.3', 'pluggy': '1.3.0'}, 'Plugins': {'asyncio': '0.23.7', 'html': '4.1.1', 'metadata': '3.1.1', 'byu_pytest_utils': '0.7.14'}}
          config: <_pytest.config.Config object at 0x10c9d9790>
      finish pytest_metadata --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.LFPlugin object at 0x10ca8b850>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.NFPlugin object at 0x10d1cfad0>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
    early skip of rewriting module: encodings.unicode_escape [assertion]
      pytest_plugin_registered [hook]
          plugin: <pytest_html.report.Report object at 0x10ca70a50>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
    early skip of rewriting module: faulthandler [assertion]
      pytest_plugin_registered [hook]
          plugin: <class '_pytest.legacypath.LegacyTmpdirPlugin'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
    early skip of rewriting module: pdb [assertion]
    early skip of rewriting module: cmd [assertion]
    early skip of rewriting module: code [assertion]
    early skip of rewriting module: codeop [assertion]
      pytest_plugin_registered [hook]
          plugin: <_pytest.config.PytestPluginManager object at 0x10c732510>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.config.Config object at 0x10c9d9790>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.mark' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/mark/__init__.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.main' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/main.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.runner' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/runner.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.fixtures' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/fixtures.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.helpconfig' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/helpconfig.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.python' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/python.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.terminal' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/terminal.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.debugging' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/debugging.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.unittest' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/unittest.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.capture' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/capture.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.skipping' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/skipping.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.legacypath' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/legacypath.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.tmpdir' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/tmpdir.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.monkeypatch' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/monkeypatch.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.recwarn' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/recwarn.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.pastebin' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/pastebin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.nose' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/nose.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.assertion' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/assertion/__init__.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.junitxml' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/junitxml.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.doctest' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/doctest.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.cacheprovider' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/cacheprovider.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.freeze_support' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/freeze_support.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.setuponly' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/setuponly.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.setupplan' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/setupplan.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.stepwise' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/stepwise.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.warnings' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/warnings.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.logging' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/logging.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.reports' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/reports.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.python_path' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/python_path.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.unraisableexception' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/unraisableexception.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.threadexception' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/threadexception.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.faulthandler' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/faulthandler.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_asyncio.plugin' from '/Users/prestonraab/.local/lib/python3.11/site-packages/pytest_asyncio/plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_html.plugin' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest_html/plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_html.fixtures' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest_html/fixtures.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_metadata.plugin' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest_metadata/plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'byu_pytest_utils.pytest_plugin' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/pytest_plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io.TextIOWrapper name="<_io.FileIO name=6 mode='rb+' closefd=True>" mode='r+' encoding='utf-8'>> err=<FDCapture 2 oldfd=7 _state='suspended' tmpfile=<_io.TextIOWrapper name="<_io.FileIO name=8 mode='rb+' closefd=True>" mode='r+' encoding='utf-8'>> in_=<FDCapture 0 oldfd=3 _state='started' tmpfile=<_io.TextIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.LFPlugin object at 0x10ca8b850>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.NFPlugin object at 0x10d1cfad0>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <pytest_html.report.Report object at 0x10ca70a50>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <class '_pytest.legacypath.LegacyTmpdirPlugin'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.terminal.TerminalReporter object at 0x10d1b4b90>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.logging.LoggingPlugin object at 0x10d1eaa10>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
    finish pytest_configure --> [] [hook]
    pytest_sessionstart [hook]
        session: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
      pytest_plugin_registered [hook]
          plugin: <_pytest.config.PytestPluginManager object at 0x10c732510>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.config.Config object at 0x10c9d9790>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.mark' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/mark/__init__.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.main' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/main.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.runner' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/runner.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.fixtures' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/fixtures.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.helpconfig' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/helpconfig.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.python' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/python.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.terminal' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/terminal.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.debugging' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/debugging.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.unittest' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/unittest.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.capture' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/capture.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.skipping' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/skipping.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.legacypath' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/legacypath.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.tmpdir' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/tmpdir.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.monkeypatch' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/monkeypatch.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.recwarn' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/recwarn.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.pastebin' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/pastebin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.nose' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/nose.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.assertion' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/assertion/__init__.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.junitxml' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/junitxml.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.doctest' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/doctest.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.cacheprovider' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/cacheprovider.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.freeze_support' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/freeze_support.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.setuponly' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/setuponly.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.setupplan' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/setupplan.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.stepwise' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/stepwise.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.warnings' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/warnings.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.logging' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/logging.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.reports' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/reports.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.python_path' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/python_path.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.unraisableexception' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/unraisableexception.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.threadexception' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/threadexception.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module '_pytest.faulthandler' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/faulthandler.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_asyncio.plugin' from '/Users/prestonraab/.local/lib/python3.11/site-packages/pytest_asyncio/plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_html.plugin' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest_html/plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_html.fixtures' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest_html/fixtures.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'pytest_metadata.plugin' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest_metadata/plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <module 'byu_pytest_utils.pytest_plugin' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/pytest_plugin.py'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=5 _state='suspended' tmpfile=<_io.TextIOWrapper name="<_io.FileIO name=6 mode='rb+' closefd=True>" mode='r+' encoding='utf-8'>> err=<FDCapture 2 oldfd=7 _state='suspended' tmpfile=<_io.TextIOWrapper name="<_io.FileIO name=8 mode='rb+' closefd=True>" mode='r+' encoding='utf-8'>> in_=<FDCapture 0 oldfd=3 _state='started' tmpfile=<_io.TextIOWrapper name='/dev/null' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.LFPlugin object at 0x10ca8b850>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.cacheprovider.NFPlugin object at 0x10d1cfad0>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <pytest_html.report.Report object at 0x10ca70a50>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <class '_pytest.legacypath.LegacyTmpdirPlugin'>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.terminal.TerminalReporter object at 0x10d1b4b90>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.logging.LoggingPlugin object at 0x10d1eaa10>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_plugin_registered [hook]
          plugin: <_pytest.fixtures.FixtureManager object at 0x10c9e00d0>
          manager: <_pytest.config.PytestPluginManager object at 0x10c732510>
      finish pytest_plugin_registered --> [] [hook]
      pytest_html_report_title [hook]
          report: <pytest_html.report_data.ReportData object at 0x10cf68c90>
      finish pytest_html_report_title --> [] [hook]
      pytest_html_results_table_header [hook]
          cells: ['<th class="sortable" data-column-type="result">Result</th>', '<th class="sortable" data-column-type="testId">Test</th>', '<th class="sortable" data-column-type="duration">Duration</th>', '<th>Links</th>']
      finish pytest_html_results_table_header --> [] [hook]
      pytest_report_header [hook]
          config: <_pytest.config.Config object at 0x10c9d9790>
          start_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests
          startdir: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests
      finish pytest_report_header --> [['asyncio: mode=Mode.STRICT'], ['rootdir: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests', 'plugins: asyncio-0.23.7, html-4.1.1, metadata-3.1.1, byu_pytest_utils-0.7.14'], ['using: pytest-7.4.3', 'setuptools registered plugins:', '  pytest-asyncio-0.23.7 at /Users/prestonraab/.local/lib/python3.11/site-packages/pytest_asyncio/plugin.py', '  pytest-html-4.1.1 at /Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest_html/plugin.py', '  pytest-html-4.1.1 at /Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest_html/fixtures.py', '  pytest-metadata-3.1.1 at /Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest_metadata/plugin.py', '  byu_pytest_utils-0.7.14 at /Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/pytest_plugin.py']] [hook]
    finish pytest_sessionstart --> [] [hook]
    pytest_collection [hook]
        session: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
    perform_collect <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0> ['/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests'] [collection]
        pytest_collectstart [hook]
            collector: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
        finish pytest_collectstart --> [] [hook]
        pytest_make_collect_report [hook]
            collector: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
        processing argument (PosixPath('/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests'), []) [collection]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/basic_text_output.observed.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/basic_text_output.observed.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/basic_text_output.observed.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/basic_text_output.observed.txt
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/hello_world.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/hello_world.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/hello_world.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/hello_world.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/ping.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/ping.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/ping.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/ping.txt
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/pong.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/pong.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/pong.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/pong.txt
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/pytestdebug.log
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/pytestdebug.log
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/pytestdebug.log
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/pytestdebug.log
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/rebuild.sh
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/rebuild.sh
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/rebuild.sh
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/rebuild.sh
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/report.html
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/report.html
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/report.html
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/report.html
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/results.json
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/results.json
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/results.json
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/results.json
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_fails.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_fails.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_fails.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_fails.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_passes.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_passes.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_passes.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_passes.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_infinite_loop.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_infinite_loop.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_infinite_loop.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_infinite_loop.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_that_writes_to_file.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_that_writes_to_file.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_that_writes_to_file.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_that_writes_to_file.py
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_basic_utils.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_basic_utils.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_basic_utils.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_basic_utils.py
              pytest_pycollect_makemodule [hook]
                  parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                  module_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_basic_utils.py
                  path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_basic_utils.py
              finish pytest_pycollect_makemodule --> <Module test_basic_utils.py> [hook]
            finish pytest_collect_file --> [<Module test_basic_utils.py>] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_cached_asset.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_cached_asset.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_cached_asset.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_cached_asset.py
              pytest_pycollect_makemodule [hook]
                  parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                  module_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_cached_asset.py
                  path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_cached_asset.py
              finish pytest_pycollect_makemodule --> <Module test_cached_asset.py> [hook]
            finish pytest_collect_file --> [<Module test_cached_asset.py>] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_exec_framework.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_exec_framework.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_exec_framework.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_exec_framework.py
              pytest_pycollect_makemodule [hook]
                  parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                  module_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_exec_framework.py
                  path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_exec_framework.py
              finish pytest_pycollect_makemodule --> <Module test_dialog_exec_framework.py> [hook]
            finish pytest_collect_file --> [<Module test_dialog_exec_framework.py>] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_framework.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_framework.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_framework.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_framework.py
              pytest_pycollect_makemodule [hook]
                  parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                  module_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_framework.py
                  path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_framework.py
              finish pytest_pycollect_makemodule --> <Module test_dialog_framework.py> [hook]
            finish pytest_collect_file --> [<Module test_dialog_framework.py>] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_infinite_loop_detection.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_infinite_loop_detection.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_infinite_loop_detection.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_infinite_loop_detection.py
              pytest_pycollect_makemodule [hook]
                  parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                  module_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_infinite_loop_detection.py
                  path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_infinite_loop_detection.py
              finish pytest_pycollect_makemodule --> <Module test_infinite_loop_detection.py> [hook]
            finish pytest_collect_file --> [<Module test_infinite_loop_detection.py>] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_run_api.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_run_api.py
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_run_api.py
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_run_api.py
              pytest_pycollect_makemodule [hook]
                  parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                  module_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_run_api.py
                  path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_run_api.py
              finish pytest_pycollect_makemodule --> <Module test_run_api.py> [hook]
            finish pytest_collect_file --> [<Module test_run_api.py>] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/.pytest_cache
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/.pytest_cache
            finish pytest_ignore_collect --> True [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/assets
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/assets
            finish pytest_ignore_collect --> None [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/assets/style.css
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/assets/style.css
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/assets/style.css
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/assets/style.css
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files
            finish pytest_ignore_collect --> None [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/basic_text_input_file.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/basic_text_input_file.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/basic_text_input_file.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/basic_text_input_file.txt
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/basic_text_output.dialog.expected.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/basic_text_output.dialog.expected.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/basic_text_output.dialog.expected.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/basic_text_output.dialog.expected.txt
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/basic_text_output.expected.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/basic_text_output.expected.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/basic_text_output.expected.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/basic_text_output.expected.txt
            finish pytest_collect_file --> [] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_cached_asset.dialog.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_cached_asset.dialog.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_cached_asset.dialog.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_cached_asset.dialog.txt
            finish pytest_collect_file --> [<DoctestTextfile test_files/test_cached_asset.dialog.txt>] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_dialog_expects_less_input.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_dialog_expects_less_input.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_dialog_expects_less_input.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_dialog_expects_less_input.txt
            finish pytest_collect_file --> [<DoctestTextfile test_files/test_dialog_expects_less_input.txt>] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_dialog_expects_more_input.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_dialog_expects_more_input.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_dialog_expects_more_input.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_dialog_expects_more_input.txt
            finish pytest_collect_file --> [<DoctestTextfile test_files/test_dialog_expects_more_input.txt>] [hook]
            pytest_ignore_collect [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                collection_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_dialog_should_pass.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_dialog_should_pass.txt
            finish pytest_ignore_collect --> None [hook]
            pytest_collect_file [hook]
                parent: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
                file_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_dialog_should_pass.txt
                path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files/test_dialog_should_pass.txt
            finish pytest_collect_file --> [<DoctestTextfile test_files/test_dialog_should_pass.txt>] [hook]
        finish pytest_make_collect_report --> <CollectReport '' lenresult=10 outcome='passed'> [hook]
        pytest_collectreport [hook]
            report: <CollectReport '' lenresult=10 outcome='passed'>
        finish pytest_collectreport --> [] [hook]
    genitems <Module test_basic_utils.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_basic_utils.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_basic_utils.py>
      find_module called for: test_basic_utils [assertion]
      matched test file '/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_basic_utils.py' [assertion]
      found cached rewritten pyc for /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_basic_utils.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_utils.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_utils.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_utils.py>
            name: pytest
            obj: <module 'pytest' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_utils.py>
            name: max_score
            obj: <function max_score at 0x10d1559e0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_utils.py>
            name: test_should_pass
            obj: <function test_should_pass at 0x10d20e0c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10d201610>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_should_pass>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_utils.py>
            name: test_should_fail
            obj: <function test_should_fail at 0x10d20e160>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10c9f3110>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_should_fail>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_basic_utils.py>
            name: __pytest_asyncio_scoped_event_loop
            obj: <function pytest_collectstart.<locals>.scoped_event_loop at 0x10d20db20>
        finish pytest_pycollect_makeitem --> None [hook]
      finish pytest_make_collect_report --> <CollectReport 'test_basic_utils.py' lenresult=2 outcome='passed'> [hook]
    genitems <Function test_should_pass> [collection]
      pytest_itemcollected [hook]
          item: <Function test_should_pass>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_should_fail> [collection]
      pytest_itemcollected [hook]
          item: <Function test_should_fail>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'test_basic_utils.py' lenresult=2 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_cached_asset.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_cached_asset.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_cached_asset.py>
      find_module called for: test_cached_asset [assertion]
      matched test file '/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_cached_asset.py' [assertion]
      found cached rewritten pyc for /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_cached_asset.py [assertion]
      early skip of rewriting module: pkgutil [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cached_asset.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cached_asset.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cached_asset.py>
            name: cache
            obj: <function cache at 0x10d155bc0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cached_asset.py>
            name: dialog
            obj: <function dialog at 0x10d156b60>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cached_asset.py>
            name: dialog_exec
            obj: <function dialog_exec at 0x10d156ac0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cached_asset.py>
            name: test_files
            obj: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cached_asset.py>
            name: python_script
            obj: <function python_script at 0x10db365c0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cached_asset.py>
            name: test_python_cached_asset
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10db367a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10db1e390>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: stdout
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_python_cached_asset[stdout]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cached_asset.py>
            name: test_exec_cached_asset
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10db368e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10d1b45d0>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: stdout
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_exec_cached_asset[stdout]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_cached_asset.py>
            name: __pytest_asyncio_scoped_event_loop
            obj: <function pytest_collectstart.<locals>.scoped_event_loop at 0x10d20de40>
        finish pytest_pycollect_makeitem --> None [hook]
      finish pytest_make_collect_report --> <CollectReport 'test_cached_asset.py' lenresult=2 outcome='passed'> [hook]
    genitems <Function test_python_cached_asset[stdout]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_python_cached_asset[stdout]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_exec_cached_asset[stdout]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_exec_cached_asset[stdout]>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'test_cached_asset.py' lenresult=2 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_dialog_exec_framework.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_dialog_exec_framework.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_dialog_exec_framework.py>
      find_module called for: test_dialog_exec_framework [assertion]
      matched test file '/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_exec_framework.py' [assertion]
      found cached rewritten pyc for /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_exec_framework.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: pytest
            obj: <module 'pytest' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: dialog_exec
            obj: <function dialog_exec at 0x10d156ac0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: max_score
            obj: <function max_score at 0x10d1559e0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: test_files
            obj: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: test_dialog_should_pass
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10d20d1c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10db93990>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: stdout
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_dialog_should_pass[stdout]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: test_dialog_should_fail
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10db36840>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10d0f2290>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: stdout
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_dialog_should_fail[stdout]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: test_dialog_expects_more_input_should_fail
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10db363e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10d0f3410>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: stdout
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_dialog_expects_more_input_should_fail[stdout]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: test_dialog_expects_less_input_should_fail
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10c0c7ba0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10d0f2d50>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: stdout
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_dialog_expects_less_input_should_fail[stdout]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: test_dialog_output_file
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10dc20cc0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10d0f2010>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: basic_text_output.expected.txt
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_dialog_output_file[basic_text_output.expected.txt]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: test_dialog_output_file_groups
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10dc209a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10dbfb910>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: basic_text_output.dialog.expected.txt
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_exec_framework.py>
            name: __pytest_asyncio_scoped_event_loop
            obj: <function pytest_collectstart.<locals>.scoped_event_loop at 0x10d20d580>
        finish pytest_pycollect_makeitem --> None [hook]
      finish pytest_make_collect_report --> <CollectReport 'test_dialog_exec_framework.py' lenresult=6 outcome='passed'> [hook]
    genitems <Function test_dialog_should_pass[stdout]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_dialog_should_pass[stdout]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_dialog_should_fail[stdout]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_dialog_should_fail[stdout]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_dialog_expects_more_input_should_fail[stdout]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_dialog_expects_more_input_should_fail[stdout]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_dialog_expects_less_input_should_fail[stdout]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_dialog_expects_less_input_should_fail[stdout]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_dialog_output_file[basic_text_output.expected.txt]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'test_dialog_exec_framework.py' lenresult=6 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_dialog_framework.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_dialog_framework.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_dialog_framework.py>
      find_module called for: test_dialog_framework [assertion]
      matched test file '/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_framework.py' [assertion]
      found cached rewritten pyc for /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_dialog_framework.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: pytest
            obj: <module 'pytest' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: dialog
            obj: <function dialog at 0x10d156b60>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: max_score
            obj: <function max_score at 0x10d1559e0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: test_files
            obj: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: test_dialog_should_pass
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10dc21580>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10db93150>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: stdout
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_dialog_should_pass[stdout]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: test_dialog_should_fail
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10dc21300>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10dbfb7d0>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: stdout
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_dialog_should_fail[stdout]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: test_dialog_expects_more_input_should_fail
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10dc214e0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10d0f2e10>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: stdout
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_dialog_expects_more_input_should_fail[stdout]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: test_dialog_expects_less_input_should_fail
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10dc20d60>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10d0f08d0>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: stdout
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_dialog_expects_less_input_should_fail[stdout]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: test_dialog_output_file
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10dc21760>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10dc37450>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: basic_text_output.expected.txt
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_dialog_output_file[basic_text_output.expected.txt]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: test_dialog_output_file_groups
            obj: <function _make_group_stats_decorator.<locals>.decorator.<locals>.new_func at 0x10dc216c0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10dc34f90>
            pytest_make_parametrize_id [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                val: basic_text_output.dialog.expected.txt
                argname: group_name
            finish pytest_make_parametrize_id --> None [hook]
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_dialog_framework.py>
            name: __pytest_asyncio_scoped_event_loop
            obj: <function pytest_collectstart.<locals>.scoped_event_loop at 0x10dc20a40>
        finish pytest_pycollect_makeitem --> None [hook]
      finish pytest_make_collect_report --> <CollectReport 'test_dialog_framework.py' lenresult=6 outcome='passed'> [hook]
    genitems <Function test_dialog_should_pass[stdout]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_dialog_should_pass[stdout]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_dialog_should_fail[stdout]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_dialog_should_fail[stdout]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_dialog_expects_more_input_should_fail[stdout]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_dialog_expects_more_input_should_fail[stdout]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_dialog_expects_less_input_should_fail[stdout]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_dialog_expects_less_input_should_fail[stdout]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_dialog_output_file[basic_text_output.expected.txt]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]> [collection]
      pytest_itemcollected [hook]
          item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'test_dialog_framework.py' lenresult=6 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_infinite_loop_detection.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_infinite_loop_detection.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_infinite_loop_detection.py>
      find_module called for: test_infinite_loop_detection [assertion]
      matched test file '/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_infinite_loop_detection.py' [assertion]
      found cached rewritten pyc for /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_infinite_loop_detection.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_infinite_loop_detection.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_infinite_loop_detection.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_infinite_loop_detection.py>
            name: pytest
            obj: <module 'pytest' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/pytest/__init__.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_infinite_loop_detection.py>
            name: _run_exec_with_io
            obj: <function _run_exec_with_io at 0x10d156700>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_infinite_loop_detection.py>
            name: this_folder
            obj: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_infinite_loop_detection.py>
            name: test_infinite_loop_detection
            obj: <function test_infinite_loop_detection at 0x10dc222a0>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10dc39290>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Coroutine test_infinite_loop_detection>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_infinite_loop_detection.py>
            name: __pytest_asyncio_scoped_event_loop
            obj: <function pytest_collectstart.<locals>.scoped_event_loop at 0x10dc213a0>
        finish pytest_pycollect_makeitem --> None [hook]
      finish pytest_make_collect_report --> <CollectReport 'test_infinite_loop_detection.py' lenresult=1 outcome='passed'> [hook]
    genitems <Coroutine test_infinite_loop_detection> [collection]
      pytest_itemcollected [hook]
          item: <Coroutine test_infinite_loop_detection>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'test_infinite_loop_detection.py' lenresult=1 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <Module test_run_api.py> [collection]
      pytest_collectstart [hook]
          collector: <Module test_run_api.py>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <Module test_run_api.py>
      find_module called for: test_run_api [assertion]
      matched test file '/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_run_api.py' [assertion]
      found cached rewritten pyc for /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_run_api.py [assertion]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_run_api.py>
            name: @py_builtins
            obj: <module 'builtins' (built-in)>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_run_api.py>
            name: @pytest_ar
            obj: <module '_pytest.assertion.rewrite' from '/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/_pytest/assertion/rewrite.py'>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_run_api.py>
            name: run_exec
            obj: <function run_exec at 0x10d156980>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_run_api.py>
            name: max_score
            obj: <function max_score at 0x10d1559e0>
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_run_api.py>
            name: test_files
            obj: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/test_files
        finish pytest_pycollect_makeitem --> None [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_run_api.py>
            name: test_run_exec
            obj: <function test_run_exec at 0x10dc22020>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10dc35a10>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_exec>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_run_api.py>
            name: test_run_exec_with_failed_test
            obj: <function test_run_exec_with_failed_test at 0x10dc22160>
          pytest_generate_tests [hook]
              metafunc: <_pytest.python.Metafunc object at 0x10dc395d0>
          finish pytest_generate_tests --> [] [hook]
        finish pytest_pycollect_makeitem --> [<Function test_run_exec_with_failed_test>] [hook]
        pytest_pycollect_makeitem [hook]
            collector: <Module test_run_api.py>
            name: __pytest_asyncio_scoped_event_loop
            obj: <function pytest_collectstart.<locals>.scoped_event_loop at 0x10dc20fe0>
        finish pytest_pycollect_makeitem --> None [hook]
      finish pytest_make_collect_report --> <CollectReport 'test_run_api.py' lenresult=2 outcome='passed'> [hook]
    genitems <Function test_run_exec> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_exec>
      finish pytest_itemcollected --> [] [hook]
    genitems <Function test_run_exec_with_failed_test> [collection]
      pytest_itemcollected [hook]
          item: <Function test_run_exec_with_failed_test>
      finish pytest_itemcollected --> [] [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'test_run_api.py' lenresult=2 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <DoctestTextfile test_files/test_cached_asset.dialog.txt> [collection]
      pytest_collectstart [hook]
          collector: <DoctestTextfile test_files/test_cached_asset.dialog.txt>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <DoctestTextfile test_files/test_cached_asset.dialog.txt>
      early skip of rewriting module: doctest [assertion]
      finish pytest_make_collect_report --> <CollectReport 'test_files/test_cached_asset.dialog.txt' lenresult=0 outcome='passed'> [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'test_files/test_cached_asset.dialog.txt' lenresult=0 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <DoctestTextfile test_files/test_dialog_expects_less_input.txt> [collection]
      pytest_collectstart [hook]
          collector: <DoctestTextfile test_files/test_dialog_expects_less_input.txt>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <DoctestTextfile test_files/test_dialog_expects_less_input.txt>
      finish pytest_make_collect_report --> <CollectReport 'test_files/test_dialog_expects_less_input.txt' lenresult=0 outcome='passed'> [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'test_files/test_dialog_expects_less_input.txt' lenresult=0 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <DoctestTextfile test_files/test_dialog_expects_more_input.txt> [collection]
      pytest_collectstart [hook]
          collector: <DoctestTextfile test_files/test_dialog_expects_more_input.txt>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <DoctestTextfile test_files/test_dialog_expects_more_input.txt>
      finish pytest_make_collect_report --> <CollectReport 'test_files/test_dialog_expects_more_input.txt' lenresult=0 outcome='passed'> [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'test_files/test_dialog_expects_more_input.txt' lenresult=0 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
    genitems <DoctestTextfile test_files/test_dialog_should_pass.txt> [collection]
      pytest_collectstart [hook]
          collector: <DoctestTextfile test_files/test_dialog_should_pass.txt>
      finish pytest_collectstart --> [] [hook]
      pytest_make_collect_report [hook]
          collector: <DoctestTextfile test_files/test_dialog_should_pass.txt>
      finish pytest_make_collect_report --> <CollectReport 'test_files/test_dialog_should_pass.txt' lenresult=0 outcome='passed'> [hook]
      pytest_collectreport [hook]
          report: <CollectReport 'test_files/test_dialog_should_pass.txt' lenresult=0 outcome='passed'>
      finish pytest_collectreport --> [] [hook]
      pytest_collection_modifyitems [hook]
          session: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
          config: <_pytest.config.Config object at 0x10c9d9790>
          items: [<Function test_should_pass>, <Function test_should_fail>, <Function test_python_cached_asset[stdout]>, <Function test_exec_cached_asset[stdout]>, <Function test_dialog_should_pass[stdout]>, <Function test_dialog_should_fail[stdout]>, <Function test_dialog_expects_more_input_should_fail[stdout]>, <Function test_dialog_expects_less_input_should_fail[stdout]>, <Function test_dialog_output_file[basic_text_output.expected.txt]>, <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>, <Function test_dialog_should_pass[stdout]>, <Function test_dialog_should_fail[stdout]>, <Function test_dialog_expects_more_input_should_fail[stdout]>, <Function test_dialog_expects_less_input_should_fail[stdout]>, <Function test_dialog_output_file[basic_text_output.expected.txt]>, <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>, <Coroutine test_infinite_loop_detection>, <Function test_run_exec>, <Function test_run_exec_with_failed_test>]
      finish pytest_collection_modifyitems --> [] [hook]
      pytest_collection_finish [hook]
          session: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
        pytest_report_collectionfinish [hook]
            config: <_pytest.config.Config object at 0x10c9d9790>
            items: [<Function test_should_pass>, <Function test_should_fail>, <Function test_python_cached_asset[stdout]>, <Function test_exec_cached_asset[stdout]>, <Function test_dialog_should_pass[stdout]>, <Function test_dialog_should_fail[stdout]>, <Function test_dialog_expects_more_input_should_fail[stdout]>, <Function test_dialog_expects_less_input_should_fail[stdout]>, <Function test_dialog_output_file[basic_text_output.expected.txt]>, <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>, <Function test_dialog_should_pass[stdout]>, <Function test_dialog_should_fail[stdout]>, <Function test_dialog_expects_more_input_should_fail[stdout]>, <Function test_dialog_expects_less_input_should_fail[stdout]>, <Function test_dialog_output_file[basic_text_output.expected.txt]>, <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>, <Coroutine test_infinite_loop_detection>, <Function test_run_exec>, <Function test_run_exec_with_failed_test>]
            start_path: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests
            startdir: /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests
        finish pytest_report_collectionfinish --> [] [hook]
      finish pytest_collection_finish --> [] [hook]
    finish pytest_collection --> None [hook]
    pytest_runtestloop [hook]
        session: <Session tests exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=19>
      pytest_runtest_protocol [hook]
          item: <Function test_should_pass>
          nextitem: <Function test_should_fail>
        pytest_runtest_logstart [hook]
            nodeid: test_basic_utils.py::test_should_pass
            location: ('test_basic_utils.py', 4, 'test_should_pass')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_should_pass>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_should_pass>>
          finish pytest_fixture_setup --> <asyncio.unix_events._UnixDefaultEventLoopPolicy object at 0x10ca8bd90> [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_should_pass>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_basic_utils.py::test_should_pass' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_basic_utils.py::test_should_pass' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_basic_utils.py::test_should_pass' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_should_pass>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_should_pass>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_should_pass>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_basic_utils.py::test_should_pass' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_basic_utils.py::test_should_pass' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_basic_utils.py::test_should_pass' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_should_pass>
            nextitem: <Function test_should_fail>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_should_pass>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_basic_utils.py::test_should_pass' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_basic_utils.py::test_should_pass' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_basic_utils.py::test_should_pass' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.00040083302883431315
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_basic_utils.py::test_should_pass' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_basic_utils.py::test_should_pass::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_basic_utils.py::test_should_pass' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0007390000391751528
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_basic_utils.py::test_should_pass' when='call' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_basic_utils.py::test_should_pass</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_basic_utils.py::test_should_pass' when='call' outcome='passed'>
              data: ['----------------------------- Captured stdout call -----------------------------\npass!\n']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00012691697338595986
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_basic_utils.py::test_should_pass' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_basic_utils.py::test_should_pass::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_basic_utils.py::test_should_pass' when='teardown' outcome='passed'>
              data: ['----------------------------- Captured stdout call -----------------------------\npass!\n']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_basic_utils.py::test_should_pass
            location: ('test_basic_utils.py', 4, 'test_should_pass')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_should_fail>
          nextitem: <Function test_python_cached_asset[stdout]>
        pytest_runtest_logstart [hook]
            nodeid: test_basic_utils.py::test_should_fail
            location: ('test_basic_utils.py', 11, 'test_should_fail')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_should_fail>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_should_fail>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_basic_utils.py::test_should_fail' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_basic_utils.py::test_should_fail' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_basic_utils.py::test_should_fail' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_should_fail>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_should_fail>
            pytest_assertrepr_compare [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                op: ==
                left: 7
                right: 8
            finish pytest_assertrepr_compare --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_should_fail>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError('assert 7 == 8') tblen=25>>
        finish pytest_runtest_makereport --> <TestReport 'test_basic_utils.py::test_should_fail' when='call' outcome='failed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_basic_utils.py::test_should_fail' when='call' outcome='failed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_basic_utils.py::test_should_fail' when='call' outcome='failed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_exception_interact [hook]
            node: <Function test_should_fail>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError('assert 7 == 8') tblen=25>>
            report: <TestReport 'test_basic_utils.py::test_should_fail' when='call' outcome='failed'>
        finish pytest_exception_interact --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_should_fail>
            nextitem: <Function test_python_cached_asset[stdout]>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_should_fail>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_basic_utils.py::test_should_fail' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_basic_utils.py::test_should_fail' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_basic_utils.py::test_should_fail' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.00014145899331197143
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_basic_utils.py::test_should_fail' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_basic_utils.py::test_should_fail::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_basic_utils.py::test_should_fail' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0006224589888006449
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_basic_utils.py::test_should_fail' when='call' outcome='failed'>
              cells: ['<td class="col-result">Failed</td>', '<td class="col-testId">test_basic_utils.py::test_should_fail</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_basic_utils.py::test_should_fail' when='call' outcome='failed'>
              data: ['@max_score(20)\n    def test_should_fail():\n&gt;       assert 7 == 8\nE       assert 7 == 8\n\ntest_basic_utils.py:14: AssertionError\n']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00015670800348743796
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_basic_utils.py::test_should_fail' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_basic_utils.py::test_should_fail::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_basic_utils.py::test_should_fail' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_basic_utils.py::test_should_fail
            location: ('test_basic_utils.py', 11, 'test_should_fail')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_python_cached_asset[stdout]>
          nextitem: <Function test_exec_cached_asset[stdout]>
        pytest_runtest_logstart [hook]
            nodeid: test_cached_asset.py::test_python_cached_asset[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_python_cached_asset[stdout]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_python_cached_asset[stdout]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_python_cached_asset[stdout]>>
          finish pytest_fixture_setup --> stdout [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_python_cached_asset[stdout]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_python_cached_asset[stdout]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_python_cached_asset[stdout]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_python_cached_asset[stdout]>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_python_cached_asset[stdout]>
            nextitem: <Function test_exec_cached_asset[stdout]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_python_cached_asset[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_python_cached_asset[stdout]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.0002555419923737645
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_cached_asset.py::test_python_cached_asset[stdout]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0005102079594507813
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='call' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_cached_asset.py::test_python_cached_asset[stdout]</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='call' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0001218329998664558
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_cached_asset.py::test_python_cached_asset[stdout]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_cached_asset.py::test_python_cached_asset[stdout]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_cached_asset.py::test_python_cached_asset[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_python_cached_asset[stdout]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_exec_cached_asset[stdout]>
          nextitem: <Function test_dialog_should_pass[stdout]>
        pytest_runtest_logstart [hook]
            nodeid: test_cached_asset.py::test_exec_cached_asset[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_exec_cached_asset[stdout]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_exec_cached_asset[stdout]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_exec_cached_asset[stdout]>>
          finish pytest_fixture_setup --> stdout [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_exec_cached_asset[stdout]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_exec_cached_asset[stdout]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_exec_cached_asset[stdout]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_exec_cached_asset[stdout]>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_exec_cached_asset[stdout]>
            nextitem: <Function test_dialog_should_pass[stdout]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_exec_cached_asset[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_exec_cached_asset[stdout]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.00019745895406231284
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_cached_asset.py::test_exec_cached_asset[stdout]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00044220895506441593
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='call' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_cached_asset.py::test_exec_cached_asset[stdout]</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='call' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0001212499919347465
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_cached_asset.py::test_exec_cached_asset[stdout]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_cached_asset.py::test_exec_cached_asset[stdout]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_cached_asset.py::test_exec_cached_asset[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_exec_cached_asset[stdout]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_dialog_should_pass[stdout]>
          nextitem: <Function test_dialog_should_fail[stdout]>
        pytest_runtest_logstart [hook]
            nodeid: test_dialog_exec_framework.py::test_dialog_should_pass[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_should_pass[stdout]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_dialog_should_pass[stdout]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_should_pass[stdout]>>
          finish pytest_fixture_setup --> stdout [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_should_pass[stdout]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_dialog_should_pass[stdout]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_dialog_should_pass[stdout]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_should_pass[stdout]>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_dialog_should_pass[stdout]>
            nextitem: <Function test_dialog_should_fail[stdout]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_should_pass[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_should_pass[stdout]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.0001931249862536788
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_should_pass[stdout]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0004337909631431103
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='call' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_should_pass[stdout]</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='call' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00010754098184406757
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_should_pass[stdout]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_pass[stdout]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_dialog_exec_framework.py::test_dialog_should_pass[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_should_pass[stdout]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_dialog_should_fail[stdout]>
          nextitem: <Function test_dialog_expects_more_input_should_fail[stdout]>
        pytest_runtest_logstart [hook]
            nodeid: test_dialog_exec_framework.py::test_dialog_should_fail[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_should_fail[stdout]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_dialog_should_fail[stdout]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_should_fail[stdout]>>
          finish pytest_fixture_setup --> stdout [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_should_fail[stdout]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_dialog_should_fail[stdout]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_dialog_should_fail[stdout]>
            pytest_assertrepr_compare [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                op: ==
                left: My args are ['script_for_dialog_fails.py', 'woot', '7', 'foobar']
Number:7
The banana is 7
Number:8
Another number is 234

                right: My args are ['script_for_dialog_passes.py', 'woot', '7']
Number: 7
The number is 7
Number: 8
Another number is 8

            finish pytest_assertrepr_compare --> [["'My args are ...mber is 234\\n' == 'My args are ...number is 8\\n'", "- My args are ['script_for_dialog_passes.py', 'woot', '7']", '?                                 ^ ^^^', "+ My args are ['script_for_dialog_fails.py', 'woot', '7', 'foobar']", '?                                 ^ ^^                  ++++++++++', '- Number: 7', '?        -', '+ Number:7', '- The number is 7', '+ The banana is 7', '- Number: 8', '?        -', '+ Number:8', '- Another number is 8', '?                   ^', '+ Another number is 234', '?                   ^^^']] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_should_fail[stdout]>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError("assert 'My args are ...mber is 234\\n' == 'My args are ...number is 8\\n'\n  - My args are ['script_fo...+++\n  - Number: 7\n  ?        -\n  + Number:7...\n  \n  ...Full output truncated (9 lines hidden), use '-vv' to show") tblen=25>>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_exception_interact [hook]
            node: <Function test_dialog_should_fail[stdout]>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError("assert 'My args are ...mber is 234\\n' == 'My args are ...number is 8\\n'\n  - My args are ['script_fo...+++\n  - Number: 7\n  ?        -\n  + Number:7...\n  \n  ...Full output truncated (9 lines hidden), use '-vv' to show") tblen=25>>
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'>
        finish pytest_exception_interact --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_dialog_should_fail[stdout]>
            nextitem: <Function test_dialog_expects_more_input_should_fail[stdout]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_should_fail[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_should_fail[stdout]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.00017166603356599808
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_should_fail[stdout]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.001481832005083561
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'>
              cells: ['<td class="col-result">Failed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_should_fail[stdout]</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'>
              data: ['group_name = &#x27;stdout&#x27;\n\n    def new_func(group_name):\n        group_stat = group_stats[group_name]\n        if not group_stat[&#x27;passed&#x27;]:\n&gt;           assert group_stat[&#x27;observed&#x27;] == group_stat[&#x27;expected&#x27;]\nE           AssertionError: assert &#x27;My args are ...mber is 234\\n&#x27; == &#x27;My args are ...number is 8\\n&#x27;\nE             - My args are [&#x27;script_for_dialog_passes.py&#x27;, &#x27;woot&#x27;, &#x27;7&#x27;]\nE             ?                                 ^ ^^^\nE             + My args are [&#x27;script_for_dialog_fails.py&#x27;, &#x27;woot&#x27;, &#x27;7&#x27;, &#x27;foobar&#x27;]\nE             ?                                 ^ ^^                  ++++++++++\nE             - Number: 7\nE             ?        -\nE             + Number:7...\nE             \nE             ...Full output truncated (9 lines hidden), use &#x27;-vv&#x27; to show\n\n../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py:29: AssertionError\n']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00025345798349007964
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_should_fail[stdout]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_dialog_exec_framework.py::test_dialog_should_fail[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_should_fail[stdout]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_dialog_expects_more_input_should_fail[stdout]>
          nextitem: <Function test_dialog_expects_less_input_should_fail[stdout]>
        pytest_runtest_logstart [hook]
            nodeid: test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_expects_more_input_should_fail[stdout]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_dialog_expects_more_input_should_fail[stdout]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_expects_more_input_should_fail[stdout]>>
          finish pytest_fixture_setup --> stdout [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_expects_more_input_should_fail[stdout]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_dialog_expects_more_input_should_fail[stdout]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_dialog_expects_more_input_should_fail[stdout]>
            pytest_assertrepr_compare [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                op: ==
                left: My args are ['script_for_dialog_passes.py', 'woot']
Number: 7
The number is 7
Number: 8
Another number is 8

Error: The program exited before all inputs were provided
                right: My args are ['script_for_dialog_passes.py', 'woot']
Number: 7
The number is 7
Number: 8
Another number is 8
Number: 9
Yep: 9

            finish pytest_assertrepr_compare --> [["'My args are ...were provided' == 'My args are ...: 9\\nYep: 9\\n'", 'Skipping 98 identical leading characters in diff, use -v to show', '  mber is 8', '- Number: 9', '- Yep: 9', '+ ', '+ Error: The program exited before all inputs were provided']] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_expects_more_input_should_fail[stdout]>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError("assert 'My args are ...were provided' == 'My args are ...: 9\\nYep: 9\\n'\n  Skipping 98 identical lea...o show\n    mber is 8\n  - Number: 9\n  - Yep: 9\n  + \n  + Error: The program exited before all inputs were provided") tblen=25>>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_exception_interact [hook]
            node: <Function test_dialog_expects_more_input_should_fail[stdout]>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError("assert 'My args are ...were provided' == 'My args are ...: 9\\nYep: 9\\n'\n  Skipping 98 identical lea...o show\n    mber is 8\n  - Number: 9\n  - Yep: 9\n  + \n  + Error: The program exited before all inputs were provided") tblen=25>>
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'>
        finish pytest_exception_interact --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_dialog_expects_more_input_should_fail[stdout]>
            nextitem: <Function test_dialog_expects_less_input_should_fail[stdout]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_expects_more_input_should_fail[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_expects_more_input_should_fail[stdout]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.0005015420028939843
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]::setup</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00100937596289441
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'>
              cells: ['<td class="col-result">Failed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'>
              data: ['group_name = &#x27;stdout&#x27;\n\n    def new_func(group_name):\n        group_stat = group_stats[group_name]\n        if not group_stat[&#x27;passed&#x27;]:\n&gt;           assert group_stat[&#x27;observed&#x27;] == group_stat[&#x27;expected&#x27;]\nE           AssertionError: assert &#x27;My args are ...were provided&#x27; == &#x27;My args are ...: 9\\nYep: 9\\n&#x27;\nE             Skipping 98 identical leading characters in diff, use -v to show\nE               mber is 8\nE             - Number: 9\nE             - Yep: 9\nE             + \nE             + Error: The program exited before all inputs were provided\n\n../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py:29: AssertionError\n']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00015066697960719466
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_expects_more_input_should_fail[stdout]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_dialog_expects_less_input_should_fail[stdout]>
          nextitem: <Function test_dialog_output_file[basic_text_output.expected.txt]>
        pytest_runtest_logstart [hook]
            nodeid: test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_expects_less_input_should_fail[stdout]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_dialog_expects_less_input_should_fail[stdout]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_expects_less_input_should_fail[stdout]>>
          finish pytest_fixture_setup --> stdout [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_expects_less_input_should_fail[stdout]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_dialog_expects_less_input_should_fail[stdout]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_dialog_expects_less_input_should_fail[stdout]>
            pytest_assertrepr_compare [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                op: ==
                left: My args are ['script_for_dialog_passes.py', 'woot']
Number: 7
The number is 7
Number: Traceback (most recent call last):
  File "/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_passes.py", line 12, in <module>
    num = get_number()
          ^^^^^^^^^^^^
  File "/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_passes.py", line 5, in get_number
    return input('Number: ')
           ^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line

Error: The program returned a non-zero exit code: 1
                right: My args are ['script_for_dialog_passes.py', 'woot']
Number: 7
The number is 7
  
            finish pytest_assertrepr_compare --> [["'My args are ... exit code: 1' == 'My args are ...mber is 7\\n  '", 'Skipping 68 identical leading characters in diff, use -v to show', '  mber is 7', '-   ', '+ Number: Traceback (most recent call last):', '+   File "/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_passes.py", line 12, in <module>', '+     num = get_number()', '+           ^^^^^^^^^^^^', '+   File "/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_passes.py", line 5, in get_number', "+     return input('Number: ')", '+            ^^^^^^^^^^^^^^^^^', '+ EOFError: EOF when reading a line', '+ ', '+ Error: The program returned a non-zero exit code: 1']] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_expects_less_input_should_fail[stdout]>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError('assert \'My args are ... exit code: 1\' == \'My args are ...mber is 7\\n  \'\n  Skipping 68 identical ...m = get_number()\n  +           ^^^^^^^^^^^^...\n  \n  ...Full output truncated (6 lines hidden), use \'-vv\' to show') tblen=25>>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_exception_interact [hook]
            node: <Function test_dialog_expects_less_input_should_fail[stdout]>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError('assert \'My args are ... exit code: 1\' == \'My args are ...mber is 7\\n  \'\n  Skipping 68 identical ...m = get_number()\n  +           ^^^^^^^^^^^^...\n  \n  ...Full output truncated (6 lines hidden), use \'-vv\' to show') tblen=25>>
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'>
        finish pytest_exception_interact --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_dialog_expects_less_input_should_fail[stdout]>
            nextitem: <Function test_dialog_output_file[basic_text_output.expected.txt]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_expects_less_input_should_fail[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_expects_less_input_should_fail[stdout]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.00020654202671721578
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0008147089974954724
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'>
              cells: ['<td class="col-result">Failed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'>
              data: ['group_name = &#x27;stdout&#x27;\n\n    def new_func(group_name):\n        group_stat = group_stats[group_name]\n        if not group_stat[&#x27;passed&#x27;]:\n&gt;           assert group_stat[&#x27;observed&#x27;] == group_stat[&#x27;expected&#x27;]\nE           assert &#x27;My args are ... exit code: 1&#x27; == &#x27;My args are ...mber is 7\\n  &#x27;\nE             Skipping 68 identical leading characters in diff, use -v to show\nE               mber is 7\nE             -   \nE             + Number: Traceback (most recent call last):\nE             +   File &quot;/Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_dialog_passes.py&quot;, line 12, in &lt;module&gt;\nE             +     num = get_number()\nE             +           ^^^^^^^^^^^^...\nE             \nE             ...Full output truncated (6 lines hidden), use &#x27;-vv&#x27; to show\n\n../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py:29: AssertionError\n']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0002108750049956143
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_expects_less_input_should_fail[stdout]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
          nextitem: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
        pytest_runtest_logstart [hook]
            nodeid: test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_output_file[basic_text_output.expected.txt]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_output_file[basic_text_output.expected.txt]>>
          finish pytest_fixture_setup --> basic_text_output.expected.txt [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_dialog_output_file[basic_text_output.expected.txt]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
            nextitem: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_output_file[basic_text_output.expected.txt]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.0002199159935116768
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00046637404011562467
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='call' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='call' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00010658300016075373
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_dialog_exec_framework.py::test_dialog_output_file[basic_text_output.expected.txt]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_output_file[basic_text_output.expected.txt]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
          nextitem: <Function test_dialog_should_pass[stdout]>
        pytest_runtest_logstart [hook]
            nodeid: test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>>
          finish pytest_fixture_setup --> basic_text_output.dialog.expected.txt [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
            nextitem: <Function test_dialog_should_pass[stdout]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.0001884579542092979
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0004343329928815365
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='call' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='call' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00011329102562740445
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_dialog_exec_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_dialog_should_pass[stdout]>
          nextitem: <Function test_dialog_should_fail[stdout]>
        pytest_runtest_logstart [hook]
            nodeid: test_dialog_framework.py::test_dialog_should_pass[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_should_pass[stdout]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_dialog_should_pass[stdout]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_should_pass[stdout]>>
          finish pytest_fixture_setup --> stdout [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_should_pass[stdout]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_dialog_should_pass[stdout]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_dialog_should_pass[stdout]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_should_pass[stdout]>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_dialog_should_pass[stdout]>
            nextitem: <Function test_dialog_should_fail[stdout]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_should_pass[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_should_pass[stdout]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.0001878330367617309
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_should_pass[stdout]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00040716701187193394
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='call' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_should_pass[stdout]</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='call' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00010608398588374257
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_should_pass[stdout]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_pass[stdout]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_dialog_framework.py::test_dialog_should_pass[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_should_pass[stdout]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_dialog_should_fail[stdout]>
          nextitem: <Function test_dialog_expects_more_input_should_fail[stdout]>
        pytest_runtest_logstart [hook]
            nodeid: test_dialog_framework.py::test_dialog_should_fail[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_should_fail[stdout]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_dialog_should_fail[stdout]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_should_fail[stdout]>>
          finish pytest_fixture_setup --> stdout [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_should_fail[stdout]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_dialog_should_fail[stdout]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_dialog_should_fail[stdout]>
            pytest_assertrepr_compare [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                op: ==
                left: My args are ['script_for_dialog_fails.py', 'woot', '7', 'foobar']
Number:7
The banana is 7
Number:8
Another number is 234

                right: My args are ['script_for_dialog_passes.py', 'woot', '7']
Number: 7
The number is 7
Number: 8
Another number is 8

            finish pytest_assertrepr_compare --> [["'My args are ...mber is 234\\n' == 'My args are ...number is 8\\n'", "- My args are ['script_for_dialog_passes.py', 'woot', '7']", '?                                 ^ ^^^', "+ My args are ['script_for_dialog_fails.py', 'woot', '7', 'foobar']", '?                                 ^ ^^                  ++++++++++', '- Number: 7', '?        -', '+ Number:7', '- The number is 7', '+ The banana is 7', '- Number: 8', '?        -', '+ Number:8', '- Another number is 8', '?                   ^', '+ Another number is 234', '?                   ^^^']] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_should_fail[stdout]>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError("assert 'My args are ...mber is 234\\n' == 'My args are ...number is 8\\n'\n  - My args are ['script_fo...+++\n  - Number: 7\n  ?        -\n  + Number:7...\n  \n  ...Full output truncated (9 lines hidden), use '-vv' to show") tblen=25>>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_exception_interact [hook]
            node: <Function test_dialog_should_fail[stdout]>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError("assert 'My args are ...mber is 234\\n' == 'My args are ...number is 8\\n'\n  - My args are ['script_fo...+++\n  - Number: 7\n  ?        -\n  + Number:7...\n  \n  ...Full output truncated (9 lines hidden), use '-vv' to show") tblen=25>>
            report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'>
        finish pytest_exception_interact --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_dialog_should_fail[stdout]>
            nextitem: <Function test_dialog_expects_more_input_should_fail[stdout]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_should_fail[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_should_fail[stdout]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.00016179197700694203
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_should_fail[stdout]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.001240332960151136
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'>
              cells: ['<td class="col-result">Failed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_should_fail[stdout]</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'>
              data: ['group_name = &#x27;stdout&#x27;\n\n    def new_func(group_name):\n        group_stat = group_stats[group_name]\n        if not group_stat[&#x27;passed&#x27;]:\n&gt;           assert group_stat[&#x27;observed&#x27;] == group_stat[&#x27;expected&#x27;]\nE           AssertionError: assert &#x27;My args are ...mber is 234\\n&#x27; == &#x27;My args are ...number is 8\\n&#x27;\nE             - My args are [&#x27;script_for_dialog_passes.py&#x27;, &#x27;woot&#x27;, &#x27;7&#x27;]\nE             ?                                 ^ ^^^\nE             + My args are [&#x27;script_for_dialog_fails.py&#x27;, &#x27;woot&#x27;, &#x27;7&#x27;, &#x27;foobar&#x27;]\nE             ?                                 ^ ^^                  ++++++++++\nE             - Number: 7\nE             ?        -\nE             + Number:7...\nE             \nE             ...Full output truncated (9 lines hidden), use &#x27;-vv&#x27; to show\n\n../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py:29: AssertionError\n']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0001742909662425518
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_should_fail[stdout]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_dialog_framework.py::test_dialog_should_fail[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_should_fail[stdout]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_dialog_expects_more_input_should_fail[stdout]>
          nextitem: <Function test_dialog_expects_less_input_should_fail[stdout]>
        pytest_runtest_logstart [hook]
            nodeid: test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_expects_more_input_should_fail[stdout]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_dialog_expects_more_input_should_fail[stdout]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_expects_more_input_should_fail[stdout]>>
          finish pytest_fixture_setup --> stdout [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_expects_more_input_should_fail[stdout]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_dialog_expects_more_input_should_fail[stdout]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_dialog_expects_more_input_should_fail[stdout]>
            pytest_assertrepr_compare [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                op: ==
                left: My args are ['script_for_dialog_passes.py', 'woot']
Number: 7
The number is 7
Number: 8
Another number is 8

                right: My args are ['script_for_dialog_passes.py', 'woot']
Number: 7
The number is 7
Number: 8
Another number is 8
Number: 9
Yep: 9

            finish pytest_assertrepr_compare --> [["'My args are ...number is 8\\n' == 'My args are ...: 9\\nYep: 9\\n'", 'Skipping 97 identical leading characters in diff, use -v to show', '  umber is 8', '- Number: 9', '- Yep: 9']] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_expects_more_input_should_fail[stdout]>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError("assert 'My args are ...number is 8\\n' == 'My args are ...: 9\\nYep: 9\\n'\n  Skipping 97 identical leading characters in diff, use -v to show\n    umber is 8\n  - Number: 9\n  - Yep: 9") tblen=25>>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_exception_interact [hook]
            node: <Function test_dialog_expects_more_input_should_fail[stdout]>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError("assert 'My args are ...number is 8\\n' == 'My args are ...: 9\\nYep: 9\\n'\n  Skipping 97 identical leading characters in diff, use -v to show\n    umber is 8\n  - Number: 9\n  - Yep: 9") tblen=25>>
            report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'>
        finish pytest_exception_interact --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_dialog_expects_more_input_should_fail[stdout]>
            nextitem: <Function test_dialog_expects_less_input_should_fail[stdout]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_expects_more_input_should_fail[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_expects_more_input_should_fail[stdout]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.0001947919954545796
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.000626500928774476
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'>
              cells: ['<td class="col-result">Failed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'>
              data: ['group_name = &#x27;stdout&#x27;\n\n    def new_func(group_name):\n        group_stat = group_stats[group_name]\n        if not group_stat[&#x27;passed&#x27;]:\n&gt;           assert group_stat[&#x27;observed&#x27;] == group_stat[&#x27;expected&#x27;]\nE           AssertionError: assert &#x27;My args are ...number is 8\\n&#x27; == &#x27;My args are ...: 9\\nYep: 9\\n&#x27;\nE             Skipping 97 identical leading characters in diff, use -v to show\nE               umber is 8\nE             - Number: 9\nE             - Yep: 9\n\n../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py:29: AssertionError\n']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00017449998995289207
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_expects_more_input_should_fail[stdout]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_dialog_expects_less_input_should_fail[stdout]>
          nextitem: <Function test_dialog_output_file[basic_text_output.expected.txt]>
        pytest_runtest_logstart [hook]
            nodeid: test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_expects_less_input_should_fail[stdout]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_dialog_expects_less_input_should_fail[stdout]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_expects_less_input_should_fail[stdout]>>
          finish pytest_fixture_setup --> stdout [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_expects_less_input_should_fail[stdout]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_dialog_expects_less_input_should_fail[stdout]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_dialog_expects_less_input_should_fail[stdout]>
            pytest_assertrepr_compare [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                op: ==
                left: My args are ['script_for_dialog_passes.py', 'woot']
Number: 7
The number is 7
Number: 
Exception: input() called more times than expected
  File "script_for_dialog_passes.py", line 12, in <module>
    num = get_number()
          ^^^^^^^^^^^^
  File "script_for_dialog_passes.py", line 5, in get_number
    return input('Number: ')
           ^^^^^^^^^^^^^^^^^
  File "/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py", line 378, in _py_input
    raise Exception("input() called more times than expected")
Exception: input() called more times than expected

                right: My args are ['script_for_dialog_passes.py', 'woot']
Number: 7
The number is 7
  
            finish pytest_assertrepr_compare --> [["'My args are ...an expected\\n' == 'My args are ...mber is 7\\n  '", 'Skipping 68 identical leading characters in diff, use -v to show', '  mber is 7', '-   ', '+ Number: ', '+ Exception: input() called more times than expected', '+   File "script_for_dialog_passes.py", line 12, in <module>', '+     num = get_number()', '+           ^^^^^^^^^^^^', '+   File "script_for_dialog_passes.py", line 5, in get_number', "+     return input('Number: ')", '+            ^^^^^^^^^^^^^^^^^', '+   File "/Users/prestonraab/opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py", line 378, in _py_input', '+     raise Exception("input() called more times than expected")', '+ Exception: input() called more times than expected']] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_expects_less_input_should_fail[stdout]>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError('assert \'My args are ...an expected\\n\' == \'My args are ...mber is 7\\n  \'\n  Skipping 68 identical... 12, in <module>\n  +     num = get_number()...\n  \n  ...Full output truncated (7 lines hidden), use \'-vv\' to show') tblen=25>>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_exception_interact [hook]
            node: <Function test_dialog_expects_less_input_should_fail[stdout]>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError('assert \'My args are ...an expected\\n\' == \'My args are ...mber is 7\\n  \'\n  Skipping 68 identical... 12, in <module>\n  +     num = get_number()...\n  \n  ...Full output truncated (7 lines hidden), use \'-vv\' to show') tblen=25>>
            report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'>
        finish pytest_exception_interact --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_dialog_expects_less_input_should_fail[stdout]>
            nextitem: <Function test_dialog_output_file[basic_text_output.expected.txt]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_expects_less_input_should_fail[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_expects_less_input_should_fail[stdout]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.0002115829847753048
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0009316669893451035
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'>
              cells: ['<td class="col-result">Failed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'>
              data: ['group_name = &#x27;stdout&#x27;\n\n    def new_func(group_name):\n        group_stat = group_stats[group_name]\n        if not group_stat[&#x27;passed&#x27;]:\n&gt;           assert group_stat[&#x27;observed&#x27;] == group_stat[&#x27;expected&#x27;]\nE           assert &#x27;My args are ...an expected\\n&#x27; == &#x27;My args are ...mber is 7\\n  &#x27;\nE             Skipping 68 identical leading characters in diff, use -v to show\nE               mber is 7\nE             -   \nE             + Number: \nE             + Exception: input() called more times than expected\nE             +   File &quot;script_for_dialog_passes.py&quot;, line 12, in &lt;module&gt;\nE             +     num = get_number()...\nE             \nE             ...Full output truncated (7 lines hidden), use &#x27;-vv&#x27; to show\n\n../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py:29: AssertionError\n']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0002268339740112424
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_expects_less_input_should_fail[stdout]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
          nextitem: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
        pytest_runtest_logstart [hook]
            nodeid: test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_output_file[basic_text_output.expected.txt]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_output_file[basic_text_output.expected.txt]>>
          finish pytest_fixture_setup --> basic_text_output.expected.txt [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_dialog_output_file[basic_text_output.expected.txt]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
            nextitem: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_output_file[basic_text_output.expected.txt]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_output_file[basic_text_output.expected.txt]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.00020116602536290884
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00043583300430327654
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='call' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='call' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00010741699952632189
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_dialog_framework.py::test_dialog_output_file[basic_text_output.expected.txt]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_output_file[basic_text_output.expected.txt]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
          nextitem: <Coroutine test_infinite_loop_detection>
        pytest_runtest_logstart [hook]
            nodeid: test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>>
          finish pytest_fixture_setup --> basic_text_output.dialog.expected.txt [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
            nextitem: <Coroutine test_infinite_loop_detection>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='group_name' scope='function' baseid=''>
              request: <SubRequest 'group_name' for <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.00017737504094839096
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00041558308294042945
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='call' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='call' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.00010712502989917994
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]' when='teardown' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_dialog_framework.py::test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]
            location: ('../../../../opt/anaconda3/envs/cs110-fall2023/lib/python3.11/site-packages/byu_pytest_utils/dialog.py', 25, 'test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Coroutine test_infinite_loop_detection>
          nextitem: <Function test_run_exec>
        pytest_runtest_logstart [hook]
            nodeid: test_infinite_loop_detection.py::test_infinite_loop_detection
            location: ('test_infinite_loop_detection.py', 5, 'test_infinite_loop_detection')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Coroutine test_infinite_loop_detection>
          pytest_fixture_setup [hook]
              fixturedef: <FixtureDef argname='event_loop' scope='function' baseid=''>
              request: <SubRequest 'event_loop' for <Coroutine test_infinite_loop_detection>>
          finish pytest_fixture_setup --> <_UnixSelectorEventLoop running=False closed=False debug=False> [hook]
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Coroutine test_infinite_loop_detection>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Coroutine test_infinite_loop_detection>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Coroutine test_infinite_loop_detection>
            pytest_assertrepr_compare [hook]
                config: <_pytest.config.Config object at 0x10c9d9790>
                op: ==
                left: 
                right: bar

            finish pytest_assertrepr_compare --> [["'' == 'bar\\n'", '- bar']] [hook]
        pytest_runtest_makereport [hook]
            item: <Coroutine test_infinite_loop_detection>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError("assert '' == 'bar\\n'\n  - bar") tblen=28>>
        finish pytest_runtest_makereport --> <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='call' outcome='failed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='call' outcome='failed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='call' outcome='failed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_exception_interact [hook]
            node: <Coroutine test_infinite_loop_detection>
            call: <CallInfo when='call' excinfo=<ExceptionInfo AssertionError("assert '' == 'bar\\n'\n  - bar") tblen=28>>
            report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='call' outcome='failed'>
        finish pytest_exception_interact --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Coroutine test_infinite_loop_detection>
            nextitem: <Function test_run_exec>
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop' scope='function' baseid=''>
              request: <SubRequest 'event_loop' for <Coroutine test_infinite_loop_detection>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Coroutine test_infinite_loop_detection>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.0005250839749351144
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_infinite_loop_detection.py::test_infinite_loop_detection::setup</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 3.0163386670174077
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='call' outcome='failed'>
              cells: ['<td class="col-result">Failed</td>', '<td class="col-testId">test_infinite_loop_detection.py::test_infinite_loop_detection</td>', '<td class="col-duration">00:00:03</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='call' outcome='failed'>
              data: ['@pytest.mark.asyncio\n    async def test_infinite_loop_detection():\n        stdout, error = await _run_exec_with_io(\n            [&#x27;python&#x27;, str(this_folder / &#x27;script_for_infinite_loop.py&#x27;)],\n            [],\n            read_timeout=1,\n            finish_timeout=3\n        )\n&gt;       assert stdout == &#x27;bar\\n&#x27;\nE       AssertionError: assert &#x27;&#x27; == &#x27;bar\\n&#x27;\nE         - bar\n\ntest_infinite_loop_detection.py:14: AssertionError\n', '----------------------------- Captured stdout call -----------------------------\npython /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_infinite_loop.py\n']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0006205830140970647
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_infinite_loop_detection.py::test_infinite_loop_detection::teardown</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='teardown' outcome='passed'>
              data: ['----------------------------- Captured stdout call -----------------------------\npython /Users/prestonraab/GitHub/cs110/byu_pytest_utils/tests/script_for_infinite_loop.py\n']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_infinite_loop_detection.py::test_infinite_loop_detection
            location: ('test_infinite_loop_detection.py', 5, 'test_infinite_loop_detection')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_exec>
          nextitem: <Function test_run_exec_with_failed_test>
        pytest_runtest_logstart [hook]
            nodeid: test_run_api.py::test_run_exec
            location: ('test_run_api.py', 3, 'test_run_exec')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_exec>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_exec>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_run_api.py::test_run_exec' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_run_api.py::test_run_exec' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_run_api.py::test_run_exec' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_exec>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_exec>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_exec>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_run_api.py::test_run_exec' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_run_api.py::test_run_exec' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_run_api.py::test_run_exec' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_exec>
            nextitem: <Function test_run_exec_with_failed_test>
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_exec>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_run_api.py::test_run_exec' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_run_api.py::test_run_exec' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_run_api.py::test_run_exec' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.00021316698985174298
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_run_api.py::test_run_exec' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_run_api.py::test_run_exec::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_run_api.py::test_run_exec' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 2.092722626053728
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_run_api.py::test_run_exec' when='call' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_run_api.py::test_run_exec</td>', '<td class="col-duration">00:00:02</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_run_api.py::test_run_exec' when='call' outcome='passed'>
              data: ['----------------------------- Captured stdout call -----------------------------\npython3 script_for_dialog_passes.py woot 7\nMy args are [&#x27;script_for_dialog_passes.py&#x27;, &#x27;woot&#x27;, &#x27;7&#x27;]\nNumber: 7\nThe number is 7\nNumber: 8\nAnother number is 8\n']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0003509170492179692
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_run_api.py::test_run_exec' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_run_api.py::test_run_exec::teardown</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_run_api.py::test_run_exec' when='teardown' outcome='passed'>
              data: ['----------------------------- Captured stdout call -----------------------------\npython3 script_for_dialog_passes.py woot 7\nMy args are [&#x27;script_for_dialog_passes.py&#x27;, &#x27;woot&#x27;, &#x27;7&#x27;]\nNumber: 7\nThe number is 7\nNumber: 8\nAnother number is 8\n']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_run_api.py::test_run_exec
            location: ('test_run_api.py', 3, 'test_run_exec')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
      pytest_runtest_protocol [hook]
          item: <Function test_run_exec_with_failed_test>
          nextitem: None
        pytest_runtest_logstart [hook]
            nodeid: test_run_api.py::test_run_exec_with_failed_test
            location: ('test_run_api.py', 10, 'test_run_exec_with_failed_test')
        finish pytest_runtest_logstart --> [] [hook]
        pytest_runtest_setup [hook]
            item: <Function test_run_exec_with_failed_test>
        finish pytest_runtest_setup --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_exec_with_failed_test>
            call: <CallInfo when='setup' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='setup' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='setup' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='setup' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_call [hook]
            item: <Function test_run_exec_with_failed_test>
          pytest_pyfunc_call [hook]
              pyfuncitem: <Function test_run_exec_with_failed_test>
          finish pytest_pyfunc_call --> True [hook]
        finish pytest_runtest_call --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_exec_with_failed_test>
            call: <CallInfo when='call' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='call' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='call' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='call' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('passed', '.', 'PASSED') [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_teardown [hook]
            item: <Function test_run_exec_with_failed_test>
            nextitem: None
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_run_exec_with_failed_test>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_run_exec>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Coroutine test_infinite_loop_detection>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_dialog_output_file[basic_text_output.expected.txt]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_dialog_expects_less_input_should_fail[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_dialog_expects_more_input_should_fail[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_dialog_should_fail[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_dialog_should_pass[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_dialog_output_file_groups[basic_text_output.dialog.expected.txt]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_dialog_output_file[basic_text_output.expected.txt]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_dialog_expects_less_input_should_fail[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_dialog_expects_more_input_should_fail[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_dialog_should_fail[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_dialog_should_pass[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_exec_cached_asset[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_python_cached_asset[stdout]>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_should_fail>>
          finish pytest_fixture_post_finalizer --> [] [hook]
          pytest_fixture_post_finalizer [hook]
              fixturedef: <FixtureDef argname='event_loop_policy' scope='session' baseid=''>
              request: <SubRequest 'event_loop_policy' for <Function test_should_pass>>
          finish pytest_fixture_post_finalizer --> [] [hook]
        finish pytest_runtest_teardown --> [] [hook]
        pytest_runtest_makereport [hook]
            item: <Function test_run_exec_with_failed_test>
            call: <CallInfo when='teardown' result: []>
        finish pytest_runtest_makereport --> <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='teardown' outcome='passed'> [hook]
        pytest_runtest_logreport [hook]
            report: <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='teardown' outcome='passed'>
          pytest_report_teststatus [hook]
              report: <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='teardown' outcome='passed'>
              config: <_pytest.config.Config object at 0x10c9d9790>
          finish pytest_report_teststatus --> ('', '', '') [hook]
          pytest_html_duration_format [hook]
              duration: 0.00032774999272078276
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='setup' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_run_api.py::test_run_exec_with_failed_test::setup</td>', '<td class="col-duration">0 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='setup' outcome='passed'>
              data: ['No log output captured.']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 2.112010083044879
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='call' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_run_api.py::test_run_exec_with_failed_test</td>', '<td class="col-duration">00:00:02</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='call' outcome='passed'>
              data: ['----------------------------- Captured stdout call -----------------------------\npython3 script_for_dialog_fails.py woot 7 foobar\nMy args are [&#x27;script_for_dialog_fails.py&#x27;, &#x27;woot&#x27;, &#x27;7&#x27;, &#x27;foobar&#x27;]\nNumber:7\nThe banana is 7\nNumber:8\nAnother number is 234\n']
          finish pytest_html_results_table_html --> [] [hook]
          pytest_html_duration_format [hook]
              duration: 0.0012263330281712115
          finish pytest_html_duration_format --> [] [hook]
          pytest_html_results_table_row [hook]
              report: <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='teardown' outcome='passed'>
              cells: ['<td class="col-result">Passed</td>', '<td class="col-testId">test_run_api.py::test_run_exec_with_failed_test::teardown</td>', '<td class="col-duration">1 ms</td>', '<td class="col-links"></td>']
          finish pytest_html_results_table_row --> [] [hook]
          pytest_html_results_table_html [hook]
              report: <TestReport 'test_run_api.py::test_run_exec_with_failed_test' when='teardown' outcome='passed'>
              data: ['----------------------------- Captured stdout call -----------------------------\npython3 script_for_dialog_fails.py woot 7 foobar\nMy args are [&#x27;script_for_dialog_fails.py&#x27;, &#x27;woot&#x27;, &#x27;7&#x27;, &#x27;foobar&#x27;]\nNumber:7\nThe banana is 7\nNumber:8\nAnother number is 234\n']
          finish pytest_html_results_table_html --> [] [hook]
        finish pytest_runtest_logreport --> [] [hook]
        pytest_runtest_logfinish [hook]
            nodeid: test_run_api.py::test_run_exec_with_failed_test
            location: ('test_run_api.py', 10, 'test_run_exec_with_failed_test')
        finish pytest_runtest_logfinish --> [] [hook]
      finish pytest_runtest_protocol --> True [hook]
    finish pytest_runtestloop --> True [hook]
    pytest_sessionfinish [hook]
        session: <Session tests exitstatus=<ExitCode.TESTS_FAILED: 1> testsfailed=8 testscollected=19>
        exitstatus: 1
      pytest_html_results_summary [hook]
          prefix: []
          summary: []
          postfix: []
          session: <Session tests exitstatus=<ExitCode.TESTS_FAILED: 1> testsfailed=8 testscollected=19>
      finish pytest_html_results_summary --> [] [hook]
      pytest_terminal_summary [hook]
          terminalreporter: <_pytest.terminal.TerminalReporter object at 0x10d1b4b90>
          exitstatus: 1
          config: <_pytest.config.Config object at 0x10c9d9790>
      early skip of rewriting module: pygments [assertion]
      early skip of rewriting module: pygments.formatters [assertion]
      early skip of rewriting module: pygments.formatters._mapping [assertion]
      early skip of rewriting module: pygments.plugin [assertion]
      early skip of rewriting module: pygments.util [assertion]
      early skip of rewriting module: pygments.formatters.terminal [assertion]
      early skip of rewriting module: pygments.formatter [assertion]
      early skip of rewriting module: pygments.styles [assertion]
      early skip of rewriting module: pygments.styles._mapping [assertion]
      early skip of rewriting module: pygments.token [assertion]
      early skip of rewriting module: pygments.console [assertion]
      early skip of rewriting module: pygments.lexers [assertion]
      early skip of rewriting module: pygments.lexers._mapping [assertion]
      early skip of rewriting module: pygments.modeline [assertion]
      early skip of rewriting module: pygments.lexers.python [assertion]
      early skip of rewriting module: pygments.lexer [assertion]
      early skip of rewriting module: pygments.filter [assertion]
      early skip of rewriting module: pygments.filters [assertion]
      early skip of rewriting module: pygments.regexopt [assertion]
      early skip of rewriting module: pygments.unistring [assertion]
        pytest_report_teststatus [hook]
            report: <TestReport 'test_basic_utils.py::test_should_fail' when='call' outcome='failed'>
            config: <_pytest.config.Config object at 0x10c9d9790>
        finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        pytest_report_teststatus [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'>
            config: <_pytest.config.Config object at 0x10c9d9790>
        finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        pytest_report_teststatus [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'>
            config: <_pytest.config.Config object at 0x10c9d9790>
        finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        pytest_report_teststatus [hook]
            report: <TestReport 'test_dialog_exec_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'>
            config: <_pytest.config.Config object at 0x10c9d9790>
        finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        pytest_report_teststatus [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_should_fail[stdout]' when='call' outcome='failed'>
            config: <_pytest.config.Config object at 0x10c9d9790>
        finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        pytest_report_teststatus [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_expects_more_input_should_fail[stdout]' when='call' outcome='failed'>
            config: <_pytest.config.Config object at 0x10c9d9790>
        finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        pytest_report_teststatus [hook]
            report: <TestReport 'test_dialog_framework.py::test_dialog_expects_less_input_should_fail[stdout]' when='call' outcome='failed'>
            config: <_pytest.config.Config object at 0x10c9d9790>
        finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
        pytest_report_teststatus [hook]
            report: <TestReport 'test_infinite_loop_detection.py::test_infinite_loop_detection' when='call' outcome='failed'>
            config: <_pytest.config.Config object at 0x10c9d9790>
        finish pytest_report_teststatus --> ('failed', 'F', 'FAILED') [hook]
      finish pytest_terminal_summary --> [] [hook]
    finish pytest_sessionfinish --> [] [hook]
    pytest_unconfigure [hook]
        config: <_pytest.config.Config object at 0x10c9d9790>
    finish pytest_unconfigure --> [] [hook]
